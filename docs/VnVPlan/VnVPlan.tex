\documentclass[12pt, titlepage]{article}

\usepackage{fullpage}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{float}
\usepackage{longtable}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{Project Title: System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Date 1 & 1.0 & Notes\\
Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

~\\

\newpage

\tableofcontents

\listoftables

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  VnV & Verification and Validation\\
  V\&V & Verification and Validation\\
  SRS & Software Requirements Specification\\
  \bottomrule
\end{tabular}\\

\newpage

\pagenumbering{arabic}

\section{General Information}

This document discusses the verification and validation plan for the REACH project. The main goal of this document
is to give all stakeholders in the REACH project confidence that the system will be built based on the correct 
requirements, and that those requirements will be implemented correctly, ensuring the system is functioning as intended.

\subsection{Summary}

The software being tested is REACH. REACH is a web application that will simplify the process for both patients, and clinicians working 
on behalf of their patients, of finding clinical trials that they are eligible for and that would be useful for them.

\subsection{Objectives}

The main objectives of this document are the following:

\begin{itemize}
  \item Ensure the system has a high level of usability
  \item Build confidence in the correctness of the searching/matching capabilities
  \item Build confidence in the security of user data
\end{itemize}

These three objectives are the most important qualities of the system, and they are the ones that are in scope for this project. \\

First, usability is a must
since the entire purpose of the application is to simplify the process of finding clinical trials. If the system was difficult to use, this process 
would not be simplified and the application would be useless. 
Furthermore, the ability to search/match patients to trials is an integral component of the application, and building confidence in 
that functionality is crucial for all stakeholders. Finally, ensuring the system is safe and secure when it comes to handling user data is of course 
extremely important for the users of the application which is why it is being prioritized.

\subsection{Relevant Documentation}

There are a few documents that are worth noting before jumping in to the verification and validation plan.
First, the SRS is the most important and relevant document that accompanies the plan  (\citet{SRS}). The majority of the test plan 
is built on top of the requirements specified in the SRS, since as stated previously, one of the main goals of this document is to 
build confidence in the implementation of the software requirements. Second, the development plan is relevant as it discusses various tools 
and technologies that can/will be used to test the software system being built {\citet{DevPlan}}. Finally, the MIS document which details the modular design 
of the system is especially relevant to the unit test portion of the document.

\section{Plan}
This section will delineate the roles of the team members as part of V\&V efforts. 
It will also describe the various verification and validation plans for the project.

\subsection{Verification and Validation Team}
\begin{table}[H]
    % \centering
    \caption{ V\&V Team}
    \begin{tabular}{|p{1.4cm}|c|p{12.5cm}|}
        \hline
        \textbf{Name} & \textbf{Team} & \textbf{Role} \\
        \hline 
        Aamina & Dev Team & As a front-end SME, Aamina will be responsible for handling functional and 
        nonfunctional testing for all front-end system capabilities. In particular, Aamina will manage 
        testing and verification pertaining to JS and TS frameworks, automated cloud deployment, graphical interface and presentation.\\
        \hline 
        David & Dev Team & As a back-end SME, David will be responsible for handling functional and nonfunctional testing 
        for all back-end system capabilities. In particular, David will manage testing and verification 
        pertaining to Python frameworks, database management, and API calls. \\
        \hline
        Anika & Dev Team & As a front-end SME, Anika will be responsible for handling functional and 
        nonfunctional testing for all front-end system capabilities. In particular, Anika will manage 
        testing and verification pertaining to JS and TS frameworks, automated cloud deployment, graphical interface and presentation.\\
        \hline
        Deep & Dev Team & As the testing SME, Deep will lead the majority of the testing efforts and ensure that the team is meeting
        the predetermined standards for testing and verification. Deep will focus on both
        front- and back-end testing and will lend his expertise to the rest of the dev team when they focus on end-to-end testing. \\
        \hline
        Alan & Dev Team & As a back-end SME, Alan will be responsible for handling functional and nonfunctional testing 
        for all back-end system capabilities. In particular, Alan will manage testing and verification 
        pertaining to Python frameworks, database management, and API calls. \\
        \hline
        All Developers & Dev Team & All developers will be responsible for writing and verifying unit, integration and 
        end-to-end tests for the system. Additionally, pipelines testing, Dockerfile testing, automated testing and any 
        other deployment-related testing is shared among all team-members.\\
        \hline
        Dr. Ho & Supervisor & As a supervisor, Dr.Ho will help the team verify the V\&V Plan and will ensure that 
        his vision is being tested properly at a high level.\\
        \hline
        Dr. Scallan & Supervisor & As a supervisor, Dr.Ho will help the team verify the V\&V Plan and will 
        ensure that his vision is being tested properly at a high level \\       
        \hline
    \end{tabular}
\end{table}

\subsection{SRS Verification Plan}

Once REACH has been created and all requirements have been implemented, 
the SRS will be verified by the following methods:
\begin{itemize}
  \item The supervisors will be given the chance to operate the system using 
  precreated inputs to generate outputs that can be compared to the expected ouput. 
  \item The supervisors will be given the chance to give 
  ad-hoc feedback on the system, and will be asked to complete a survey evaluating how
  well the development team implemented the requirements. 
  The requirements in the survey will be functional and nonfunctional ones that are easy 
  for those without a software engineering background to understand and evaluate.

  \item The development team will operate the system as well in order to assess the system's
  performance on all requirements and how well the system meets expected testing outputs.
  A survey will be filled out by the development team as well, 
  but it will be more detailed and granular.
\end{itemize}
\subsection{Design Verification Plan}

As stated in the objectives, the main purpose of REACH is to connect patients looking to be 
involved in a clinical trial with researchers looking for participants for their research studies. 
This means the usability of the design is one of the top priorities, as people with different 
abilities will be using REACH. This can include seniors, or people who are not as adept at using 
technology. For this reason, having a design which focuses on improving the user's interaction 
with the web application is vital. This can be done by confirming the usability of the design.\\

The maintainability of the design is also something that needs to be verified. Due to the fact 
that REACH is in its early stages, it is likely that new improvements and features will be added 
in the future. To achieve this, other principles such as anticipation of change, separation of 
concerns, modularity, and reusability should be accounted for.

\subsubsection{Usability}

Usability refers to how easily typical users of REACH are able to use the web application. The 
usability of the design is greatly impacted by its user interface, so we will mainly be focusing 
on verifying and validating the usability of the user interface. The usability of the design also 
directly depends on the user's capabilities and characteristics, so it is important that a range 
of users with different capabilities use REACH in order for us to verify how usable it is.\\

Our design will be reviewed by Dr. Ho and Dr. Scallan, as they are supervisors, stakeholders, and 
potential users of REACH. They will select a few other users (around three) with different 
skill sets and characteristics to also review the design. These users are expected to have at least a passable level of computer literacy insofar as they are understand how to navigate a website. They are not specifically expected to have knowledge relating to medicine. The reviewers will use the checklist 
below and keep the checklist items in mind during their review of REACH.\\

\noindent Usability Checklist:

\begin{itemize}
  \item Text, icons, and formatting are consistent
  \begin{itemize}
    \item Reduces confusion and allows users to pick up and understand meaning behind certain text and icons quicker
  \end{itemize}

  \item Only relevant information and instructions are displayed without any clutter
  \begin{itemize}
    \item Ensures user is not distracted or confused by any extra information
  \end{itemize}

  \item Visible and prompt feedback is provided to the user after they complete an action
  \begin{itemize}
    \item The system should visibly respond to both successful and unsuccessful actions made by the user and keep the user informed and updated
  \end{itemize}

  \item Error handling exists
  \begin{itemize}
    \item Simple details of what the error is, along with straightforward instructions on how the user can fix the error will cause the user to be less confused and panicked
  \end{itemize}

  \item Simple descriptions which guide users on what to do next are always available or visible to the user
  \begin{itemize}
    \item Users should not need to spend time thinking about what action they should take next
  \end{itemize}

  \item User interactions with the system should have constraints
  \begin{itemize}
    \item Restricting what the user can select, input, or interact with will limit future errors and issues the user experiences
  \end{itemize}
\end{itemize}

This review will also be aided by the \nameref{sec:useabilityNFRtests} tests listed under Tests for 
Nonfunctional Requirements, as well as the \nameref{sec:Appendix}.

\subsubsection{Maintainability}

Maintainability refers to how easily the system can be maintained, including the ease at which 
it can be improved by fixing any bugs or expanded to include new features. Multiple principles 
affects the maintainability of a system, including modularity, reusability, and whether we design 
for change. Having modules which implement the separation of concerns principle helps to pinpoint 
where an issue or bug might be located. This, along with the modules having low coupling 
(depending less on other modules), makes it easier to add new modules which map to new features 
or upgrades. The reusability principle can be implemented by having standardized and generic 
components which are reused multiple times. This will also make it easier to add new features, 
as we can reuse the components.\\

The review will involve code inspections, such as a code walkthrough, where the focus is on if 
there's a valid separation of concerns, and whether this separation clearly exists within the 
code. This code inspection will occur regularly throughout the development of REACH. Every member 
of the team will review and verify whether the design is maintainable. Some members will be 
focusing on modularity. They will ensure the code is decomposed into modules which contain 
related information. The rest of the members will focus on reusability. They will confirm that 
we are creating and using generic components whenever possible.

\subsection{Verification and Validation Plan Verification Plan}

In order to test the VnV Plan, we as a team will be inspecting this document. Some members of the 
team will thoroughly inspect specific sections of this document, while other members will be 
conducting a more general, overview level inspection of the overall document.\\

The three main sections that need to be reviewed are the verification plans, the system tests, 
and the unit tests. Three members will each select one of these main sections. They will each be 
confirming that each test and verification plan listed in their section of the document is complete, 
feasible, and unambiguous.\\

Two members of the team will take on the task of reviewing the overall document and they will 
confirm that the document contains all tests necessary. They will ensure they every requirement we 
have listed in the SRS document is covered by at least one test.\\

TAs and our peers will also assist with an overview level inspection of this document. The TA 
assigned to our team will view our VnV Plan with a specific rubric in mind, then provide us feedback 
based on the standards of this class. This rubric is available on the course's Avenue to Learn page under Assignments $>$ VnV Plan - Main. Our peers will review our VnV Plan when they are conducting 
their regular peer review and will open git issues with any concerns they have.\\

\noindent Division of Work:

\begin{itemize}
  \item Review of Verification Plans: Anika
  \item Review of System Tests: David
  \item Review of Unit Tests: Alan
  \item Review of Overall VnV Plan: Deep, Aamina, TAs, Peers
\end{itemize}



\subsection{Implementation Verification Plan}

To test the implementation first we have the tests listed below in section 4 and the unit tests listed in section 5. 
All of these tests should be run before each major update to the program (revision 0, revision 1, etc). Relevant tests should
also be run when the affected requirements are modified during minor updates. Also during minor updates the code should be 
inspected by at least one other developer using a github pull request. We will also be using static analysis, the specific 
software for this and other testing is listed in section 3.6 below.

\subsection{Automated Testing and Verification Tools}

For the backend (i.e., python service(s)), we will be using pytest as our unit testing framework. Additionally,
we will be using tavern (an API testing framework built on top of pytest) to test our API. These API tests will
ideally run at scheduled intervals (i.e., nightly, weekly... whatever is reasonable for the resources available), which can be 
done with the use of GitHub actions. Furthermore, we will be using several static analyzers to ensure code quality and maintainability. These 
static analyzers include flake8 (which ensures the code will follow the pep8 coding standard), mypy (which ensures strict typing in python,
and the use of comments+docstrings), and autoflake (which ensures there are no unused packages/imports in a python file).\\

For the frontend, nothing has changed from the development plan (i.e., using jest for unit testing and ESLint for linting).

\subsection{Software Validation Plan}

To validate our software system/requirements, there are a few steps that will be taken. First, continuing to meet with our clients (supervisors) while developing
the early parts of the application will help ensure the system is on the right track. Furthermore, upon completing of the revision 0 demo, we will
demo the system to our clients, enabling them to ask questions, provide feedback, and ensure the system has the correct requirements. At this
time we will also encourage our clients to use the platform directly (since it will be deployed, they will be able to easily access it) and give additional feedback post revision 0 demo,
which will hopefully bring light to new and improved requirements that can be implemented for revision 1. 

\section{System Test Description}
	
\subsection{Tests for Functional Requirements}

The following subsections are split based on the core aspects of the application. These include authentication, which enables a user 
to sign in to their account, user data, which enables users to save their data for future uses of the application, the emailing system, which will
notify users of new trials and give them a template for reaching out to trial clinicians, searching for trials (the main purpose of the app), and 
general program/system information (i.e., the FAQ).

\subsubsection{Authentication}

\begin{enumerate}

\item{AT-1\\}

Control: Manual
					
Initial State: User is not signed in to the application, and is about to create an account.
					
Input: A valid, unique email, unique username, and a valid password satisfying the NIST guidelines
					
Output: The user's account should be created successfully, and a 201 CREATED response should be returned.

Test Case Derivation: As stated in FR-1 and FR-2, the system must only allow a user to create an account if the email is unique (i.e., not 
used by some other user yet), and if their password satisfies the NIST guidelines. Since the inputs to this test case satisfy these 
requirements, the response should be a success.
					
How test will be performed: Pre define a valid email, username and password. Manually input these values on the registration and check the return value/output.
					
\item{AT-2\\}

Control: Manual
					
Initial State: User is not signed in to the application, and is about to create an account.
					
Input: An email that is already attached to another account, unique username, and a valid password.
					
Output: The user's account should not be created and a response stating this failure should be returned.

Test Case Derivation: From FR-2, the email must be unique. Since it is not unique in this test case, the account should not 
be created.

How test will be performed: Pre define an invalid email. Manually input these values and check the return value/output.

\item{AT-3\\}

Control: Manual
					
Initial State: User is not signed in to the application, and is about to create an account.
					
Input: A valid, unique, email, unique username, and a password that does not satisfy the NIST guidelines.
					
Output: The user's account should not be created and a response stating this failure should be returned.

Test Case Derivation: From FR-1, the password must abide by the NIST guidelines. Since it does not in this test case, the account should not 
be created.

How test will be performed: Pre define an invalid password. Manually input these values and check the return value/output.


\item{AT-4\\}

Control: Manual
					
Initial State: User is not signed in to the application, and is about to log in to the application.
					
Input: A valid unique username, and a password that satisfies the NIST guidelines, which are tied to an account.
					
Output: The user should be logged in to the application.

Test Case Derivation: From FR-3, if the credentials are correct, a user should be logged in to the application.

How test will be performed: Pre define a valid username and password tied to an account. Manually input these values and check the return value/output.


\item{AT-5\\}

Control: Manual
					
Initial State: User is not signed in to the application, and is about to log in to the application.
					
Input: A username that is not tied to an account, with a password that satisfies NIST guidelines (but can be any valid password).
					
Output: The user should not be logged in to the application.

Test Case Derivation: From FR-3, a user is only logged in to an account if the credentials are correct. Since the username is not valid,
they should not be logged in.

How test will be performed: Pre define a random username/password. Manually input these values and check the return value/output.

\item{AT-6\\}

Control: Manual
					
Initial State: User is not signed in to the application, and would like to reset their password.
					
Input: A valid email address.
					
Output: The user should receive an email, containing a link which will allow them to reset their password.

Test Case Derivation: From FR-5, a user must receive an email when attempting to reset their password.

How test will be performed: Manually go on to the login page, enter an email address, and click reset password. Then, verify
that the email received is correct, and that the link takes the user to the reset password page.


\item{AT-7\\}

Control: Manual
					
Initial State: User is not signed in to the application, and is about to reset their password.
					
Input: A password that is different from the one currently tied to their account, satisfying the NIST guidelines.
					
Output: The user's password should be reset, and they should be logged in to the application.

Test Case Derivation: From FR-5, a user must only be able to reset their password if the password they enter is new, and if it 
satisfies the NIST guidelines.

How test will be performed: Pre define a random email/password. Manually pass in these values and check the return value/output.

\item{AT-8\\}

Control: Manual
					
Initial State: User is signed in to the application and is about to sign out of the application.
					
Input: N/A
					
Output: The user should be logged out of the application.

Test Case Derivation: From FR-6, a user should be able to log out of the application, with really no constraints.

How test will be performed: Manually sign out of the application, and verify that it works.


% \item{AT-9\\}

% Control: Manual
					
% Initial State: User has created an account for the first time, or is logging in as a guest.
					
% Input: N/A
					
% Output: The user is prompted to enter their personal data.

% Test Case Derivation: From FR-7, a user is prompted to enter personal information when creating an account or logging in as a guest.

% How test will be performed: Manually create an account and sign in as a guest, and verify the user is prompted.

\end{enumerate}

\subsubsection{User Data}

\begin{enumerate}

\item{DT-1\\}

Control: Manual
					
Initial State: User is not signed in, and does not have an account.
					
Input: New user data (medical conditions, personal attributes).
					
Output: Updated user database displayed on the user profile page.
	
Test Case Derivation: From FR-8, the system shall store the information entered by the user in the database.

How test will be performed: Manually create a new account, and add in some user data and medical conditions. Log back in and verify the data is still present on the user profile.


\item{DT-2\\}

Control: Manual
					
Initial State: User is signed in with their personal account.
					
Input: Updated user data (medical conditions, personal attributes).
					
Output: Updated user database displayed on the user profile page.

Test Case Derivation: From FR-9, a user shall be able to update their personal information.

How test will be performed: Manually update the user profile with new user information, then log out. Log back in and check that the new data is present.


\item{DT-3\\}

Control: Manual
					
Initial State: Tester is not logged in.
					
Input: Database query.
					
Output: List of non-unique unique database elements.

Test Case Derivation: From FR-8, the system shall store the information entered by the user in the database.

How test will be performed: Manually query the database for columns that are supposed to contain unique values, namely keys, such as user ID, email address, and etc. Verify that the length of the returned set is 0.

Rationale: To ensure that key constraints are upheld, we must ensure that the values are non-unique, otherwise querying for certain users may return multiple sets of data.


\end{enumerate}


\subsubsection{Email}

\begin{enumerate}


\item{ET-1\\}

Control: Manual
					
Initial State: The user has searched for trials, and has been given a list of trials.
					
Input: N/A
					
Output: A customized email template.

Test Case Derivation: From FR-10, the system shall create an email template for the user.

How test will be performed: Manually select a given trial result, and select the option to generate a template. Verify that it is displayed to the user.

\end{enumerate}

\subsubsection{Program Information}

\begin{enumerate}

\item{PIT-1\\}

Control: Manual
					
Initial State: The user is at the home screen.
					
Input: N/A
					
Output: FAQ menu.

Test Case Derivation: From FR-12, the system shall include frequently asked questions and trial information.

How test will be performed: Manually select the FAQ menu, and verify that it displays the FAQ.


\end{enumerate}

\subsubsection{Searching}

\begin{enumerate}

\item{ST-1\\}

Control: Manual
					
Initial State: The user is signed is, either as themselves or as a guest.
					
Input: Medical conditions and other user attributes used to search.
					
Output: Displayed search results.

Test Case Derivation: From FR-13, FR-14, a user shall be able to search for medical studies based on their health conditions, and the system shall display to the users the results of their trial search.

How test will be performed: Manually enter the a set of medical conditions and physical attributes into the search function. Submit, and verify the results are returned.


\item{ST-2\\}

Control: Manual
					
Initial State: The user is signed into their personal account, with medical information saved to their account.
					
Input: N/A
					
Output: Displayed search results.

Test Case Derivation: From FR-15, the system shall allow a signed-in user to automatically enter their conditions into the search parameters.

How test will be performed: Manually navigate to the search function. Verify that their conditions and information is automatically inputted into the search fields.


\item{ST-3\\}

Control: Manual
					
Initial State: The user is logged in, either as themselves or as a guest, and is at the search function.
					
Input: N/A
					
Output: N/A

Test Case Derivation: From FR-15, the system shall allow a signed-in user to automatically enter their conditions into the search parameters.

How test will be performed: Attempt to search without any fields filled in. Verify that the search fails.

\item{ST-4\\}

Control: Manual
					
Initial State: The user is logged in, and is at the search trials page with a set of information currently inputted for a search.
					
Input: User selects a new set of pre-saved search criteria, and hits search.
					
Output: A list of trials which were found on the basis of the previously selected set of criteria.

Test Case Derivation: From FR-20, the system shall allow a user to switch between multiple sets of pre-saved conditions into the search parameters.

How test will be performed: Verify that the trials returned and displayed to the user are based on the most recently selected set of 
search criteria.

\item{ST-5\\}

Control: Manual
					
Initial State: The user is logged in, and has just conducted a search for trials.
					
Input: User selects one of the displayed trials.
					
Output: The map should change its location to the location of the selected trial.

Test Case Derivation: From FR-19, the system shall allow a user to view the location of the selected trial on a map.

How test will be performed: Verify that the location displayed on the map is indeed the correct location tied to the study.


\end{enumerate}


\subsubsection{Bookmarking}

\begin{enumerate}

\item{BT-1\\}

Control: Manual
					
Initial State: The user is signed in, and has conducted a search for trials.
					
Input: Bookmark a trial.
					
Output: The trial indicates that it has been saved.

Test Case Derivation: From FR-16, FR-18, a user must be able to save a trial, and access the trials that they have saved.

How test will be performed: Manually save a trial, and verify that the newly saved trial has been saved, and is available to view.

\item{BT-2\\}

Control: Manual
					
Initial State: The user is signed in, and has saved at least one trial.
					
Input: View the saved trials page.
					
Output: The display of trials that have been previously saved by the user.

Test Case Derivation: From FR-17, a user must be able to delete a trial that they have previously saved.

How test will be performed: Manually delete a trial that has been previously saved, and verify that it no longer exists with the other saved trials.

\item{BT-3\\}

Control: Manual
					
Initial State: The user is signed in, has saved at least one trial, and is on the page for viewing their saved trials.
					
Input: View trial details.
					
Output: The details of the saved trial.

Test Case Derivation: From FR-18, a user must be able to perform the same actions on the saved trials as they can on the trial results after 
conducting a search. Therefore, the same information must be displayed to the user regarding the saved trial.

How test will be performed: Manually click, or view, a saved trial and ensure the displayed information is the same as the information obtained 
on the searching page.

\item{BT-4\\}

Control: Manual
					
Initial State: The user is signed in, has saved at least one trial under multiple profiles, and is on the page for viewing their saved trials.
					
Input: Select a profile to filter the saved trials by.
					
Output: The list of trials that have been saved under that profile.

Test Case Derivation: From FR-19, a user must be able to perform the same actions on the saved trials as they can on the trial results after 
conducting a search. Therefore, the same information must be displayed to the user regarding the saved trial.

How test will be performed: Select a profile, and verify that the page updates the list of displayed trials to just the ones that were saved
under the selected profile.


\end{enumerate}


\subsection{Tests for Nonfunctional Requirements}

\subsubsection{Useability}
\label{sec:useabilityNFRtests}

The tests in this section are to be completed as part of the \nameref{sec:Appendix}.

\begin{enumerate}

\item{UT-1}

Control: Dynamic, Manual
					
Initial State: The user starts on the home screen.

Test Case Derivation: From NFR-3, NFR-4, and NFR-5, regarding the learnability, simplicity, and legibility of the user interface.

How test will be performed: Have the user navigate to the sign in page without outside help. Then ask the user to 
rate how easy the task was on a scale of 1 to 5, where 1 is difficult and 5 is simple. The test passes if the average 
rating is greater than 3.5.

\item{UT-2}

Control: Dynamic, Manual
					
Initial State: The user starts on the sign in page.

Test Case Derivation: From NFR-3, NFR-4, and NFR-5, regarding the learnability, simplicity, and legibility of the user interface.

How test will be performed: Have the user navigate to the registration page without outside help. Then ask the user to 
rate how easy the task was on a scale of 1 to 5, where 1 is difficult and 5 is simple. The test passes if the average 
rating is greater than 3.5.

\item{UT-3}

Control: Dynamic, Manual
					
Initial State: The user starts on the registration page.

Test Case Derivation: From NFR-3, NFR-4, and NFR-5, regarding the learnability, simplicity, and legibility of the user interface.

How test will be performed: Have the register a new account without outside help. Then ask the user to 
rate how easy the task was on a scale of 1 to 5, where 1 is difficult and 5 is simple. The test passes if the average 
rating is greater than 3.5.

\item{UT-4}

Control: Dynamic, Manual
					
Initial State: The user starts on the search trials page.

Test Case Derivation: From NFR-3, NFR-4, and NFR-5, regarding the learnability, simplicity, and legibility of the user interface.

How test will be performed: Have the user search for clinical trials without outside help. Then ask the user to 
rate how easy the task was on a scale of 1 to 5, where 1 is difficult and 5 is simple. The test passes if the average 
rating is greater than 3.5.

\item{UT-5}

Control: Dynamic, Manual
					
Initial State: The user starts on search trials page after a search has already been performed.

Test Case Derivation: From NFR-3, NFR-4, and NFR-5, regarding the learnability, simplicity, and legibility of the user interface.

How test will be performed: Have the user try viewing more details about a trial without outside help. Then ask the user to 
rate how easy the task was on a scale of 1 to 5, where 1 is difficult and 5 is simple. The test passes if the average 
rating is greater than 3.5.

\item{UT-6}

Control: Dynamic, Manual
					
Initial State: The user starts on search trials page after a search has already been performed.

Test Case Derivation: From NFR-3, NFR-4, and NFR-5, regarding the learnability, simplicity, and legibility of the user interface.

How test will be performed: Have the user bookmark a trial without outside help. Then ask the user to 
rate how easy the task was on a scale of 1 to 5, where 1 is difficult and 5 is simple. The test passes if the average 
rating is greater than 3.5.

\item{UT-7}

Control: Dynamic, Manual
					
Initial State: The user starts on search trials page after a search has already been performed and a trial has been bookmarked.

Test Case Derivation: From NFR-3, NFR-4, and NFR-5, regarding the learnability, simplicity, and legibility of the user interface.

How test will be performed: Have the user view the trials they bookmarked without outside help. Then ask the user to 
rate how easy the task was on a scale of 1 to 5, where 1 is difficult and 5 is simple. The test passes if the average 
rating is greater than 3.5.

\item{UT-8}

Control: Dynamic, Manual
					
Initial State: The user starts on the bookmarked trials page.

Test Case Derivation: From NFR-3, NFR-4, and NFR-5, regarding the learnability, simplicity, and legibility of the user interface.

How test will be performed: Have the user navigate back to the search trials page without outside help. Then ask the user to 
rate how easy the task was on a scale of 1 to 5, where 1 is difficult and 5 is simple. The test passes if the average 
rating is greater than 3.5.

\item{UT-9}

Control: Dynamic, Manual
					
Initial State: The user starts on the home page.

Test Case Derivation: From NFR-3, NFR-4, and NFR-5, regarding the learnability, simplicity, and legibility of the user interface.

How test will be performed: Have the user view their account and profile information without outside help. Then ask the user to 
rate how easy the task was on a scale of 1 to 5, where 1 is difficult and 5 is simple. The test passes if the average 
rating is greater than 3.5.

\item{UT-10}

Control: Dynamic, Manual
					
Initial State: The user starts on account information page.

Test Case Derivation: From NFR-3, NFR-4, and NFR-5, regarding the learnability, simplicity, and legibility of the user interface.

How test will be performed: Have the user attempt to add a new patient profile without outside help. Then ask the user to 
rate how easy the task was on a scale of 1 to 5, where 1 is difficult and 5 is simple. The test passes if the average 
rating is greater than 3.5.

\item{UT-11}

Control: Dynamic, Manual
					
Initial State: The user starts on the FAQs page.

Test Case Derivation: From NFR-3, NFR-4, and NFR-5, regarding the learnability, simplicity, and legibility of the user interface.

How test will be performed: Have the user navigate to the REACH home page without outside help. Then ask the user to 
rate how easy the task was on a scale of 1 to 5, where 1 is difficult and 5 is simple. The test passes if the average 
rating is greater than 3.5.

\item{UT-12}

Control: Dynamic, Manual
					
Initial State: The user starts on the home screen. The selected language should be set to the default (English).

Test Case Derivation: From NFR-17, regarding accessibility of the application users who do not speak an official language.

How test will be performed: Users of the next five most spoken languages other than English and French should be selected. They should be instructed to change to their preferred language and then should login, search, and browse results. 95\% of users should not report any issues navigating the interface.


\end{enumerate}

\subsubsection{Performance}
		
\begin{enumerate}

\item{PT-1\\}

Type: Dynamic, Manual
					
Initial State: User has entered their information for their and is about to search for eligible trials.
					
Input/Condition: All possible information filled in.
					
Output/Result: The system should load the trials and display them to the user in less than 5 seconds.
					
How test will be performed: In an existing account with all information categories filled in by the user, manually search for eligible trials, 
and keep track of how long it takes for the trials to load.
					
\item{PT-2\\}

Type:  Manual
					
Initial State: N/A
					
Input/Condition: API key available to authenticate against the API.
					
Output/Result: API calls should respond in less than 1 second.
					
How test will be performed: A tester will test each endpoint individually, and measure the response time. If all response times are below one second, the test passes.

\end{enumerate}

\subsubsection{Operational and Environmental}
		
\begin{enumerate}

\item{OT-1\\}

Type: Automatic
					
Initial State: N/A
					
Input/Condition: Database is populated with running trials
					
Output/Result: The system should show the number of trials within an order of magnitude to the trials on ClinicalTrials.gov
					
How test will be performed: Count the number of running trials in the database and compare to the number of 
running trials on ClinicalTrials.gov

\item{OT-2\\}

Type: Dynamic, Manual
					
Initial State: User is on home screen, on a variety of devices
					
Output/Result: The user should report satisfactory performance on all devices ($>7$ on the 10 point usability survey included in the appendix).
					
How test will be performed: Give the user multiple devices from the last 5 years (ex. iphone 15, windows laptop, chromebook, etc)
and ask them to rate the performance of the application compared to a desktop pc from 1-10.

\end{enumerate}

\subsubsection{Maintainability}

\begin{enumerate}

\item{MT-1\\}

Type: Static, Manual
					
How test will be performed: Manually run linters on code before merging a PR.

Rationale: This will ensure code is following the proper standards, which will help increase the maintainability of the system.
Also, this will be performed manually for the reason of conserving resources (i.e., could setup some workflow to automatically lint 
code every time some code is pushed to the GitHub Repo, however this could be costly).

\end{enumerate}

\subsubsection {Security}
\begin{enumerate}
  \item{SEC-1\\}
  Control: Automatic

  Initial State: User is at the account creation screen and is trying to create a new account.

  Input: User enters a password that does not satisfy the NIST guidelines listed on the creation page.
  
  Output: The user should not be able to create an account and an 
  error message will appear as feedback mentioning the recommendation to follow NIST guidelines.

  Test Derivation: From NFR-15, regarding the security of the user's passwords as per NIST guidelines.
  In particular this NFR mentions rejecting weak passwords.

  How the test will be performed: A new account will be created and an invalid password will be entered. 
  The account creation should fail and an error message explaining why account creation failed
  should appear. 
  
  Rationale: This will be automated for the purpose of testing many passwords 
  that nearly meet NIST guidelines and doing so manually would be exhausting.
  
  \item {SEC-2\\}
  Control: Manual

  Initial State: The system is running and the 
  database access settings have been configured prior to the test. 

  Input: A user without the appropriate authorization (but with the right password) credentials attempts to access the database.
  OR An authorized database manager
  with the appropriate password credentials attempts to access the database.
  
  Output: The unauthorized user should not be able to access the database and an error message will appear as feedback.
  The authorized database manager should be able to access the database and no error message should appear.

  Test Derivation: From NFR-16 the system should only allow authorized database manager to access the 
  intricacies of the database. In particular this NFR mentions rejecting unauthorized users from accessing the database.
  
  How the test will be performed: An unauthorized user will attempt to access the database with valid password credentials.
  The database access should fail and an error message explaining why access failed should appear.
  An authorized database manager will attempt to access the database with valid password credentials.
  The database access should succeed and no error message should appear.

\end{enumerate}
\subsubsection{Legal}

\begin{enumerate}
\item{LT-1}

Control: Dynamic, Manual
					
Initial State: The user is at the search screen.

Input: User search criteria.

Output: Option prompting user for consent.

Test Case Derivation: From NFR-18, regarding the collection of user consent before using or storing user data.

How test will be performed: Users are asked to input their search criteria, and should see a message prompting them to provide consent for the collection of data when they try to search.

\end{enumerate}

\subsection{Traceability Between Test Cases and Requirements}


\begin{longtable}{|p{7cm}|p{7cm}|}
  \caption{Test Case-Requirement Traceability}\\  
  \hline
  Test case & Requirement(s)\\
  \hline
  AT-1 & FR-1, FR-2\\
  \hline
  AT-2 & FR-2\\
  \hline
  AT-3 & FR-1\\
  \hline
  AT-4 & FR-3\\
  \hline
  AT-5 & FR-3\\
  \hline
  AT-6 & FR-5\\
  \hline
  AT-7 & FR-5\\
  \hline
  AT-8 & FR-6\\
  \hline
  AT-9 & FR-7\\
  \hline
  DT-1 & FR-8\\
  \hline
  DT-2 & FR-9\\
  \hline
  DT-3 & FR-8\\
  \hline
  ET-1 & FR-10\\
  \hline
  PIT-1 & FR-12\\
  \hline
  ST-1 & FR-13, FR-14\\
  \hline
  ST-2 & FR-15\\
  \hline
  ST-3 & FR-15\\
  \hline
  ST-4 & FR-21\\
  \hline
  ST-5 & FR-20\\
  \hline
  BT-1 & FR-16, FR-18\\
  \hline
  BT-2 & FR-17, FR-18\\
  \hline
  BT-3 & FR-18\\
  \hline
  BT-4 & FR-19\\
  \hline
  UT-1, UT-2, UT-3, UT-4, UT-5, UT-6, UT-7, UT-8, UT-9, UT-10, UT-11 & NFR-3, NFR-4, NFR-5\\
  \hline
  UT-12 & NFR-17\\
  \hline
  PT-1 & NFR-6\\
  \hline
  PT-2 & NFR-7\\
  \hline
  OT-1 & NFR-11\\
  \hline
  OT-2 & NFR-12\\
  \hline
  MT-1 & NFR-13\\
  \hline
  SEC-1 & NFR-15\\
  \hline
  SEC-2 & NFR-16\\
  \hline
  LT-1 & NFR-18\\
  \hline
\end{longtable}

\section{Unit Test Description}

\subsection{Unit Testing Scope}

Our app is split up by two services - a backend and frontend service. With respect to unit testing, we decided to place more emphasis on the backend service modules,
since we feel like they are more susceptible to small "edge case" errors, and therefore should be tested a bit more thoroughly. \\

The frontend is continuously being tested, both through us, the developers, and through the many people who have conducted usability tests of the app using our survey, so we feel as though
any errors in the frontend are much more likely to be found without the use of unit tests than the backend. Related to the frontend, the modules that we are not explicitly unit testing are: 
the Email Template Module, Patient Data collection form module, Login Form Module, Profile Settings Module, and the Menu Module.\\

For the backend, we treated each api endpoint that interacts with one of the data modules as a "unit test". A good chunk of the functionality specified 
in the MIS for the backend modules are provided out of the box by Django and the Django Rest Framework, so this greatly reduced the number 
of tests we needed to write (for example, any simple getter or setter of some data module attribute does not need to be tested, as Django 
implements these for us). That said, we have explicit unit tests for the following modules (links to these tests will be provided below): User Data Module (UserData),
Info Profile Data Module (PatientInfo), Trial Data Module (Trial), Fetch Trials Module (TrialFetcher), and Trial Filtering Module (TrialFilterer). The two backend 
modules that we are not explicitly testing at all are the login and registration modules, since this functionality is provided directly by Django, with no 
additional work needed from us.\\

For more information on these modules, please see the \href{https://github.com/davimang/REACH/tree/main/docs/Design/SoftDetailedDes}{Module Interface Specification} and/or \href{https://github.com/davimang/REACH/blob/main/docs/Design/SoftArchitecture/MG.pdf}{Module Guide}.

\subsection{Tests for Functional Requirements}

The following link contains the unit tests for the modules specified above: \url{https://github.com/davimang/REACH/tree/main/src/backend/reach_backend/Api/tests}.
The tests are well documented, with a descriptive name, and a docstring that specifies what the test is for, what module it is testing, and the id of the test for 
traceability purposes.

\subsection{Traceability Between Test Cases and Modules}
\begin{longtable}{|p{7cm}|p{7cm}|}
  \caption{Test Case-Module Traceability}\\ 
  \hline
  Test case & Module\\
  \hline
  UNT-1 & M3\\
  \hline
  UNT-2 & M3\\
  \hline
  UNT-3 & M3\\
  \hline
  UNT-4 & M3\\
  \hline
  UNT-5 & M2\\
  \hline
  UNT-6 & M2\\
  \hline
  UNT-7 & M1\\
  \hline
  UNT-8 & M1\\
  \hline
  UNT-9 & M4\\
  \hline
  UNT-10 & M4\\
  \hline
  UNT-11 & M4\\
  \hline
  UNT-12 & M4\\
  \hline
  UNT-13 & M4\\
  \hline
  UNT-14 & M4\\
  \hline
  UNT-15 & M5\\
  \hline
  UNT-16 & M5\\
  \hline
  UNT-17 & M5\\
  \hline
  UNT-18 & M5\\
  \hline
  UNT-19 & M5\\
  \hline
  UNT-20 & M5\\
  \hline
  UNT-21 & M4\\
  \hline
\end{longtable}

\newpage
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Test \& Survey}
\label{sec:Appendix}
\underline{Usability Pre-Test Questions}:
\begin{enumerate}
  \item What age range do you belong to?
    \begin{itemize}
      \item $<18$
      \item $18 - 35$
      \item $36 - 50$
      \item $51 - 65$
      \item $65 +$
    \end{itemize}
	\item How would you rate your proficiency using computers?
    \begin{itemize}
      \item Beginner
      \item Average
      \item Intermediate
      \item Expert
    \end{itemize}
  \item How would you rate your proficiency in typing?
    \begin{itemize}
      \item Beginner
      \item Average
      \item Intermediate
      \item Expert
    \end{itemize}
\end{enumerate}


\noindent \underline{Usability Test}:\\
\noindent \emph{Ask the user to complete the tasks listed in the table below. After each task, ask the user to rate how easy
the task was on a scale of 1 to 5, where 1 is difficult and 5 is simple.}
\begin{table}[H]
  \begin{tabular}{|p{0.32\textwidth}|p{0.12\textwidth}|p{0.12\textwidth}|p{0.12\textwidth}|p{0.12\textwidth}|p{0.12\textwidth}|}
  \hline
  \textbf{Task} & \textbf{Difficult (1)} & \textbf{Somewhat Difficult (2)} & \textbf{Okay (3)} & \textbf{Somewhat Simple (4)} & \textbf{Simple (5)} \\ \hline
  1. Navigate to the sign in page                                   & & & & & \\ \hline
  2. Navigate to the registration page                              & & & & & \\ \hline
  3. Register a new account                                         & & & & & \\ \hline
  4. Search for clinical trials                                     & & & & & \\ \hline
  5. Try viewing more details about a trial                         & & & & & \\ \hline
  6. Bookmark a trial or multiple trials                            & & & & & \\ \hline
  7. View the trials you bookmarked                                 & & & & & \\ \hline
  8. Navigate back to the search trials page                        & & & & & \\ \hline
  9. View your account and profiles information                     & & & & & \\ \hline
  10. Attempt to add a new profile                                  & & & & & \\ \hline
  11. Navigate to the REACH home page                               & & & & & \\ \hline
  \end{tabular}
\end{table}

\noindent \underline{Usability Post-Test Questions}:
\begin{enumerate}
  \item Was it easy navigating to all of the different pages? (Y/N)
    \begin{itemize}
      \item Rate how easy navigation of the site was on a scale of 1 to 10 (1 being difficult and 10 being very easy).
      \item Explain if there was a specific page that was difficult to navigate to.
    \end{itemize}
	\item Was the trial search page easy to understand and navigate? (Y/N)
    \begin{itemize}
      \item Rate how straightforward the trial search page was on a scale of 1 to 10 (1 being very confusing, 10 being straightforward).
      \item Explain what you think could improve the trial search page (if anything).
    \end{itemize}
  \item At any point was there anything that you were not sure what its purpose was? (Y/N)
    \begin{itemize}
      \item If yes, what was it?
    \end{itemize}
  \item Were you able to comfortably read all the text on the site? (Y/N)
    \begin{itemize}
      \item Explain if there was any text too small or too big.
    \end{itemize}
  \item  Is there any feature or part of the process which you found frustrating? (Y/N)
    \begin{itemize}
      \item If yes, explain what was frustrating and why.
    \end{itemize}
  \item Do you have any additional feedback or concerns? (Y/N)
    \begin{itemize}
      \item If yes, explain what part of the interface you were concerned with and why.
    \end{itemize}
\end{enumerate}

\newpage{}
\section*{Appendix --- Reflection}

Below are skills that the team will need to develop in order to successfully carry out the verification and validation process, in addition to how the team plans to further our knowledge in the given areas.

\begin{itemize}
\item Automated unit testing via integrations
\begin{itemize}
\item Free tutorials on Youtube or other free video sharing platforms.
\item Documentation on the given cloud platform's integrations.
\end{itemize}
\item Verification and validation reporting standards
\begin{itemize}
\item Lecture material and other course material, from both capstone and other courses.
\item Documentation and templates from IEEE.
\end{itemize}
\end{itemize}
For the automated unit testing, the team has decided to pursue video tutorials, as these tend to be easier to approach, and provide practical examples. For the VnV reporting, the team has decided to use course materials, as this is readily accessible and easy to understand.


Additionally, below is a skill that each member will need to develop, along with two ways they can learn each skill.
\begin{itemize}
\item Alan - GitHub CI/CD pipelines
\begin{itemize}
\item YouTube tutorials or video tutorials from other sites.
\item GitHub documentation.
\end{itemize}
\item David - Jest testing framework
\begin{itemize}
  \item Jest documentation
  \item YouTube videos/tutorials 
\end{itemize}
\item Deep - Pytest testing framework
\begin{itemize}
  \item Course website
  \item YouTube videos/tutorials 
\end{itemize}
\item Anika - Working with automated testing frameworks in pipelines
  \begin {itemize}
    \item Pytest and Jest documentation
    \item Pluralsight tutorials
  \end {itemize}
\item Aamina - Pytest testing framework
  \begin {itemize}
    \item Pytest documentation
    \item YouTube videos/tutorials
  \end {itemize}
\end{itemize}

Below is which method each member intends to use, and why.
\begin{itemize}
\item Alan - YouTube tutorials. This method is the most approachable, and provides easy to follow along examples.
\item David - Jest documentation. I will use the documentation since I have used several other testing frameworks in previous projects, and I anticipate the documentation should provide sufficient information to get up and running with the framework.
\item Deep - Course Website. This method will allow me to learn about pytest is a more formalized scenario. This will help with writing tests according to industry standards.
\item Anika - Pluralsight tutorials. I will use this method because it will help me learn about the frameworks in a more structured way with in-depth practice and examples.
\item Aamina - YouTube videos/tutorials. Since I am new to Pytest and have previously only worked with Java and the Junit testing framework, it would be best to watch videos with simple explanations and get a walkthrough of different examples.
\end{itemize}


\end{document}