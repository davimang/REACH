\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{Verification and Validation Report: \progname} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
03/04/2024 & 1.0 & Add results for some functional tests\\
03/06/2024 & 1.1 & Add results for unit tests + reflection\\
\bottomrule
\end{tabularx}

~\newpage

\section{Symbols, Abbreviations and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  UNT & Unit Test\\
  UT & Usability Test\\
  ST & Searching Test\\
  DT & Data Test\\
  BT & Bookmarking Test\\
  PIT & Program Info Test\\
  PT & Performance Test\\
  \bottomrule
\end{tabular}\\

\wss{symbols, abbreviations or acronyms -- you can reference the SRS tables if needed}

\newpage

\tableofcontents

\listoftables %if appropriate

\listoffigures %if appropriate

\newpage

\pagenumbering{arabic}

\section{Functional Requirements Evaluation}

\large{\textbf{User Data}}

\normalsize

\begin{enumerate}
  \item DT-1
  Result: \textcolor{green}{Passed} - New account can successfully be created, and data is stored successfully in the database. Therefore, the tester was able to verify that the data is 
  persistent, and so this test was successful.
  \item DT-2
  Result: \textcolor{green}{Passed} - Account information can successfully be updated, and the changes are reflected in the database. Therefore, the tester was able to verify
  that the updates are persistent, and so this test was successful.
  \item DT-3
  Result: \textcolor{gray}{Not run} - reason being that the database application we used will ensure the issue brought up by this test can never happen.\\
\end{enumerate}

\noindent{\large{\textbf{Program Information}}}

\normalsize

\begin{enumerate}
  \item PIT-1 \textcolor{green}{Passed} - The tester was able to verify that the FAQ system can be viewed by a user (whether they are logged in or not), and so this test was successful.\\
\end{enumerate}

\noindent{\large{\textbf{Searching}}}

\normalsize

\begin{enumerate}
  \item ST-1 \textcolor{gray}{Not run} - Functionality tested by this test was removed from the requirements prior to conducting testing.
  \item ST-2 \textcolor{green}{Passed} - The tester was able to conduct a search based on a patient profile, and view the resulting trials. Therefore, this test was 
  successful.
  \item ST-3 \textcolor{red}{Failed} - When attempting to search for trials with no profile data selected, the app did not behave as intended (it
  still tried to conduct a search), therefore this test was unsuccessful.
  \item ST-4 \textcolor{green}{Passed} - The tester was able to switch between multiple profiles, and conduct a search based on each respective profile.
  \item ST-5 \textcolor{green}{Passed} - Upon selecting a displayed trial, the tester was able to verify that the location displayed on the map updates and displays correctly.\\

\end{enumerate}

\noindent{\large{\textbf{Bookmarking}}}

\normalsize

\begin{enumerate}
  \item BT-1 \textcolor{green}{Passed} - The tester was able to save a trial, and view the trial in the bookmarks page. Additionally, there was an indication that
  the trial was saved, upon saving the trial. Therefore, this test was successful.
  \item BT-2 \textcolor{green}{Passed} - The tester was able to delete a trial that was previously saved, and verified that the deletion was recognized in the frontned (i.e., by 
  no longer being part of the list of saved trials). Therefore, this test was successful.
  \item BT-3 \textcolor{green}{Passed} - The tester was able to verify that the details provided by the saved trial are the same as the details related to the trial 
  on the search trial page.
  \item BT-4 \textcolor{green}{Passed} - The tester was able to verify that the saved trials are filtered correctly upon selecting a profile to filter by. When selecting a 
  profile, only the trials saved under that profile are displayed, and when selecting no profile, all saved trials are displayed to the user.

\end{enumerate}


\section{Nonfunctional Requirements Evaluation}

\subsection{Usability}
		
\subsection{Performance}
\begin{enumerate}
  \item PT-1 \textcolor{green}{Passed} - On average, the tests to load trials take on average 4.1 seconds. As this is below the benchmark of 5 seconds, the test passes.
\item PT-2 \textcolor{green}{Passed} - All endpoints were tested manually to measure the time taken to establish connections between endpoints. All tests succeeded in under 1 second, so the tests pass.
\end{enumerate}

\subsection{Maintainability}
\begin{enumerate}
  \item MT-1 \textcolor{red}{Failed} - Modified code in the pull request has been routinely and automatically linted by a Github CI component upon creation of pull requests. Some pull requests have been rejected by the linter, so the test fails.
\end{enumerate}

\subsection{Security}
\begin{enumerate}
  \item SEC-1 \textcolor{red}{Failed} - The tester entered the password "pass" which does not conform to the NIST guidelines. The password was accepted and the account was created, so the test fails.
\item SEC-2 \textcolor{green}{Passed} - Two tests were run. Firstly, a tester with the correct password but incorrect username attempted to access the database. This test was successful as the tester was denied access to the database. Secondly, a tester with correct credentials attempted to log into the database. This attempt was successful, so the second part of the test was also successful.
\end{enumerate}

	
\section{Comparison to Existing Implementation}	

This section will not be appropriate for every project.

\section{Unit Testing}

\begin{enumerate}
  \item UNT-1
  Result: \textcolor{green}{Passing} 
  \item UNT-2
  Result: \textcolor{green}{Passing}
  \item UNT-3
  Result: \textcolor{green}{Passing}
  \item UNT-4
  Result: \textcolor{green}{Passing} 
  \item UNT-5
  Result: \textcolor{green}{Passing}
  \item UNT-6
  Result: \textcolor{green}{Passing} 
  \item UNT-7
  Result: \textcolor{green}{Passing}
  \item UNT-8
  Result: \textcolor{green}{Passing}
  \item UNT-9
  Result: \textcolor{red}{Failing} - Functionality that is tested by this unit test is currently under a slight rework, which is causing the test to fail.
  \item UNT-10
  Result: \textcolor{red}{Failing} - Functionality that is tested by this unit test is currently under a slight rework which is causing the test to fail.
  \item UNT-11
  Result: \textcolor{green}{Passing}
  \item UNT-12
  Result: \textcolor{green}{Passing} 
  \item UNT-13
  Result: \textcolor{green}{Passing}
  \item UNT-14
  Result: \textcolor{green}{Passing}
  \item UNT-15
  Result: \textcolor{green}{Passing} 
  \item UNT-16
  Result: \textcolor{green}{Passing}
  \item UNT-17
  Result: \textcolor{green}{Passing}
  \item UNT-18
  Result: \textcolor{green}{Passing} 
  \item UNT-19
  Result: \textcolor{green}{Passing}
  \item UNT-20
  Result: \textcolor{green}{Passing}
  \item UNT-21
  Result: \textcolor{green}{Passing}
\end{enumerate}

\section{Changes Due to Testing}

\wss{This section should highlight how feedback from the users and from 
the supervisor (when one exists) shaped the final product.  In particular 
the feedback from the Rev 0 demo to the supervisor (or to potential users) 
should be highlighted.}

In terms of receiving feedback on our designs, our team has cultivated a diverse group of potential users 
in order ascertain that our design is meeting the requirements of the primary stakeholders. 
More specifically, throughout the term, 
we held biweekly focus meetings with Dr. Ho and Dr. Scallan where we received crucial 
pointers regarding key design issues in visibility, consistency and clarity. 
For example, a salient piece of feedback that truly helped us in the development process
had to do with limiting distances for trial searches. Prior to this feedback, 
we did not have a limit on how far away a trial could be. This potentially linked 
users to trials that could be over 500 km away from their main address. It did not 
cost us too much in terms of usability metrics (it was not a big factor) but by changing this 


\section{Automated Testing}
		
\section{Trace to Requirements}
		
\section{Trace to Modules}		

\section{Code Coverage Metrics}

\bibliographystyle{plainnat}
\bibliography{../../refs/References}

\newpage{}
\section*{Appendix --- Reflection}

The information in this section will be used to evaluate the team members on the
graduate attribute of Reflection.  Please answer the following question:

\begin{enumerate}
  \item In what ways was the Verification and Validation (VnV) Plan different
  from the activities that were actually conducted for VnV?  If there were
  differences, what changes required the modification in the plan?  Why did
  these changes occur?  Would you be able to anticipate these changes in future
  projects?  If there weren't any differences, how was your team able to clearly
  predict a feasible amount of effort and the right tasks needed to build the
  evidence that demonstrates the required quality?  (It is expected that most
  teams will have had to deviate from their original VnV Plan.)
\end{enumerate}

There were a few ways in which the VnV plan was different from the activities that were actually conducted. First, there were a few tests written 
for functionality that is no longer planned to be in the app. For example, the email notification system/module is no longer in scope, and as a result,
tests were obviously not conducted for this module. Furthermore, there were some tests that did not need to be done as a direct result of the 
implementation, and more specifically, some of the tools that were used in the implementation. For example, a good chunk of the authentication tests did 
not actually need to be done, since Django provides an out of the box solution to authenticating users (which we were able to take advantage of). It would be 
very difficult to anticipate all of these changes in a future project, since there are so many variables affecting the testing stage (i.e., change in scope,
implementation details, etc..). Fortunately, this is why we follow the iterative process, since it allows us to make continuous updates to our documentation,
ensuring there is always consistency between the plan, report, the app itself, and the other documents (such as SRS, MIS, etc..).


\end{document}